[{"content":" Static or conditional factor model? There are static or dynamic model concerning whether beta is time dependent.\nStatic model The most traditional factor model is Fama French factor model, which is the static model. That is, the factor exposure is obtained by taking time series regression of return to the factor value in whole sample, which means the factor exposure of each equity is constant in the whole sample.\n$$ r_t = \\mathbb{E}(r_t) + \\beta v_t + u_t, $$ where \\(r_t\\) is an \\(N \\times 1\\) vector of excess returns of test assets (assume we have \\(N\\) stocks in our portfolio), \\(\\beta\\) is an \\(N \\times K\\) matrix of factor exposures (assume we have \\(N \\times K\\) factors on the portfolio), \\(v_t\\) is a \\(K \\times 1\\) vector of factor innovations (assume we have \\(K\\) factors on each stock), and \\(u_t\\) is an \\(N \\times 1\\) vector of idiosyncratic errors. The expected return can (always) be decomposed as:\n$$ \\mathbb{E}(r_t) = \\alpha + \\beta \\gamma, $$ \\(\\gamma\\) and \\(v_t\\) can be expressed as: $$ f_t=\\gamma+ v_t, $$ Thus the static model can be write as: $$ \\Rightarrow r_t = \\alpha + f_t + \\varepsilon_t $$ Conditional model In reality, the factor exposure is changing with time on individual stock. Thus we need to consider the time variation on factors exposure \\(\\beta\\), which is conditional factor model as follows: $$ \\tilde{r}_t = \\alpha_{t-1} + \\beta_{t-1} \\gamma_{t-1} + \\beta_{t-1} v_t + \\tilde{u}_t $$ where \\(\\tilde{r}_t\\) and \\(\\tilde{u}_t\\) are \\(M \\times 1\\) vectors of excess returns and idiosyncratic errors of individual stocks. O We use conditional factor model to model beta by firm characteristic \\(b_{t-1}\\), \\cite{Rosenberg1974} imposes that \\(\\beta_{t-1} = b_{t-1} \\beta\\), where \\(b_{t-1}\\) is an \\(M \\times N\\) matrix of observable characteristics and \\(\\beta\\) \\(\\beta\\) is an \\(N \\times K\\) vector of parameters. Consequently, the model becomes $$ \\tilde{r}_t = b_{t-1} \\tilde{f}_t + \\tilde{\\varepsilon}_t, $$ where \\(\\tilde{f}_t := \\beta (\\gamma_{t-1} + v_t)\\) is a new \\(N \\times 1\\) vector of latent factors, and \\(\\tilde{\\varepsilon}_t := \\alpha_{t-1} + \\tilde{u}_t\\). This is the MSCI Barra model prototype which has been embraced by practitioners for its simplicity and versatility in modeling individual equity returns. Estimating factors and exposures There are 2 different methods to estimate beta(factor exposure or factor loading). One is traditional method, that is, get beta by doing time series regression. Then doing cross sectional regression to estimate factor return. Another is using firm characteristic as factor exposure directly without doing time series regression. Then doing cross sectional regression to estimate factor return.\nFor macroeconomic factors like GDP and interest rate, we can only use the first method since exposure of macroeconomic factors like interest rate is same for each different individual stock, which is not true. In reality, the exposure of certain macroeconomic factor to different stock varies from one to another. Thus, we should get factor exposure by time series regression first and then apply cross sectional regression to estimate factor return.\nStatic model Static model could be Fundamental Multi-Factor Models, which use both macroneconomic predictors such as inflation, unemployment rate, and interest rates, as well as securities-specific predictors such as market capitalization, book-to-market ratio, investment, momentum, and operating profitability.\nFor estimating \\(\\beta\\) and \\(f\\). We can use Fama macbeth method: Getting \\(\\beta\\) by time series regression: $$ \\hat{\\beta} = \\bar{r} \\bar{f}^\\top \\left( \\bar{f} \\bar{f}^\\top \\right)^{-1}, $$ then take cross sectional regression between \\(\\beta\\) and \\(r_t\\) at each \\(t\\) to estimate \\(f_t\\), then take average of \\(f_t\\): $$ \\hat{f_t} = \\left( \\beta^\\top \\beta \\right)^{-1} \\beta^\\top r_t. $$ Or we can use similar method called traditional method : Getting beta by time series regression, then take expectation of \\(r_t\\), then use cross sectional regression to estimate \\(f_t\\) based on beta and expectation of \\(r_t\\). It could also be Statistical Multi-Factor Models, which apply statistical techniques such as factor analysis to identify the return related factors. For statistical method, we use PCA to estimate beta and f:\n$$ \\bar{R} = \\sum_{j=1}^{\\hat{K}} \\sigma_j \\zeta_j \\xi_j^\\top + \\hat{U}, $$ $$ \\hat{V} = T^{1/2} \\left( \\xi_1 : \\xi_2 : \\cdots : \\xi_{\\hat{K}} \\right)^\\top, \\quad \\hat{\\beta} = T^{-1/2} \\left( \\sigma_1 \\zeta_1 : \\sigma_2 \\zeta_2 : \\cdots : \\sigma_{\\hat{K}} \\zeta_{\\hat{K}} \\right). $$ Conditional model $$ \\hat{f}_t = \\left( b_{t-1}^\\top b_{t-1} \\right)^{-1} b_{t-1}^\\top \\tilde{r}_t. $$ using cross sectional regression get f. e.g. barra model 型的目标是解释股票收益率时序波动，而非股票预期收益的截面差异；此外，采用 firm characteristics 直接做因子暴露也是很粗糙的，且模型也可能存在遗漏变量/无关变量的问题。**Then corss sectional regrerssion of firm characteristics replaces portfolio sort constructing factor mimicking portfolios.** How can we get better conditional factor model(dynamic model) has been the research interest. We mainly focus on the second type, where factors are latent. $$ \\tilde{r}_t = b_{t-1} \\beta f_t + \\tilde{\\varepsilon}_t $$ We can use machine learning method to model factor exposure \\(\\beta\\) based on observable characteristic data, e.g. IPCA models[Kelly et al. (2020)](https://arxiv.org/pdf/2307.15936) a linear approximation for factor exposures \\(\\beta\\) , where the form of \\(\\beta\\) can be estimated by minimizing pricing error of objective function. \\(\\beta\\) can be estimated from panel regressions of returns onto characteristics interacted with factors. After estimating \\(\\beta\\), factors are estimated from cross section regressions of returns on \\(\\beta\\): $$ \\text{vec}(\\hat{\\beta}^\\top) = \\left( \\sum_{t=2}^{T} b_{t-1}^\\top b_{t-1} \\otimes \\hat{f}_t \\hat{f}_t^\\top \\right)^{-1} \\left( \\sum_{t=2}^{T} \\left( b_{t-1} \\otimes \\hat{f}_t^\\top \\right) \\tilde{r}_t \\right) $$ $$ % F_{t+1} = \\left( \\beta_t^\\prime \\beta_t \\right)^{-1} \\beta_t^\\prime R_t^e \\hat{f}_t = \\left( \\hat{\\beta}^\\top b_{t-1}^\\top b_{t-1} \\hat{\\beta} \\right)^{-1} \\hat{\\beta}^\\top b_{t-1}^\\top \\tilde{r}_t $$ Based on Barra and IPCA, [(Gu et al. (2021))](https://arxiv.org/pdf/2307.15936) extended the linear \\(\\beta\\) setting with autoencoder model, where \\(\\beta\\) is regarded as nonlinear function of firm characteristic. Figure 1:Conditional Autoencoder Model\nWe can use recursive formulation for the non linear beta function:\n$$ b_{i,t-1}^{(0)} = b_{i,t-1}, $$ $$ b_{i,t-1}^{(l)} = g \\left( b^{(l-1)} + W^{(l-1)} b_{i,t-1}^{(l-1)} \\right), \\quad l = 1, ..., L_{\\beta}, $$ $$ \\beta_{i,t-1} = b^{(L_{\\beta})} + W^{(L_{\\beta})} b_{i,t-1}^{(L_{\\beta})}. $$ The recursive mathematical formulation of the factors is:\n$$ r_t^{(0)} = \\left( b_{t-1}^\\top b_{t-1} \\right)^{-1} b_{t-1}^\\top r_t, $$ $$ r_t^{(l)} = \\tilde{g} \\left( \\tilde{b}^{(l-1)} + \\tilde{W}^{(l-1)} r_t^{(l-1)} \\right), \\quad l = 1, ..., L_f, $$ $$ f_t = \\tilde{b}^{(L_f)} + \\tilde{W}^{(L_f)} r_t^{(L_f)}. $$ Estimating Risk Premia Estimating SDF Appendix $$ f_t = U_t + \\gamma_{t-1} $$ $$ \\text{Define } \\mathcal{F}_{t-1} = \\text{filtration generated by } \\gamma_{t-1} $$ $$ \\Rightarrow \\mathbb{E}_{t-1}[f_t] = \\mathbb{E}\\left[f_t \\mid \\mathcal{F}_{t-1}\\right] $$ $$ = \\mathbb{E}\\left[U_t + \\gamma_{t-1} \\mid \\mathcal{F}_{t-1}\\right] $$ $$ = \\mathbb{E}\\left[U_t \\mid \\mathcal{F}_{t-1}\\right] + \\mathbb{E}\\left[\\gamma_{t-1} \\mid \\mathcal{F}_{t-1}\\right] $$ $$ \\text{Since } U_t \\perp \\mathcal{F}_{t-1} $$ $$ \\Rightarrow \\mathbb{E}\\left[U_t \\mid \\mathcal{F}_{t-1}\\right] = \\mathbb{E}[U_t] = 0 $$ $$ \\text{and } \\gamma_{t-1} \\text{ is } \\mathcal{F}_{t-1}\\text{measurable} $$ $$ \\Rightarrow \\mathbb{E}\\left[\\gamma_{t-1} \\mid \\mathcal{F}_{t-1}\\right] = \\gamma_{t-1} $$ $$ \\Rightarrow \\mathbb{E}_{t-1}[f_t] = \\gamma_{t-1} $$ ","permalink":"https://XIAOLAN-design.github.io/posts/factor_model/","summary":"Static or conditional factor model? There are static or dynamic model concerning whether beta is time dependent.\nStatic model The most traditional factor model is Fama French factor model, which is the static model. That is, the factor exposure is obtained by taking time series regression of return to the factor value in whole sample, which means the factor exposure of each equity is constant in the whole sample.","title":"Factor Models Introduction draft"},{"content":"Courses: Generalized Linear Model, Statistical Learning, Mathematical Finance, operational research, advanced algebra, mathematical analysis,stochastic analysis, probability theory.\nInterest: Application of machine learning like large language models on finance.\nResearch Projects Realted to Mathematics and Finance Research Proposal: Systematic Risk Discovery in High-Frequency Financial Data The study aims to explore methods for identifying risk factors in high-frequency trading data, focusing on market equilibrium analysis\n• Reviewed existing research on risk-return patterns in both high and low frequency trading, with emphasis on how market microstructure affects asset returns\n• Developed new methodology combining econometric techniques with two-stage Principal Component Analysis (PCA) to reduce noise and extract risk factors from trading data\n• Showed that our approach successfully identifies market microstructure risk factors at the transaction level, enabling better risk pricing and trading strategies in high-frequency markets\nResearch Assistant at TUM Data Innovation Lab, Differential Machine Learning https://www.mdsi.tum.de/fileadmin/w00cet/di-lab/pdf/KPMG_SS2022_Final_Report.pdf\n• Investigated differential machine learning applications in Credit Value Adjustment (CVA) computation of options pricing, addressing the computational intensity challenge in derivatives valuation of the financial market\n• Developed neural network models to optimize CVA calculations, aiming to enable efficient intraday pricing and pre-deal risk assessment\n• Empirically demonstrated that differential machine learning approaches reduce computational complexity while maintaining accuracy, making CVA calculations feasible for intraday trading and risk management\n• Proposed theoretical framework for extending the methodology to multi-underlying derivatives and comprehensive CVA calculations\nQuant Intern, UniCredit Trading floor, Munich, Germany • Implemented portfolio optimization strategies in Python, designing algorithms to maximize returns under multiple trading constraints and risk limits\n• Built automated index calculation tools for systematic trading, featuring customizable risk control rules and dynamic portfolio rebalancing schedules\n• Enhanced bank\u0026rsquo;s trading infrastructure by developing Python libraries for portfolio management and risk analysis\n","permalink":"https://XIAOLAN-design.github.io/posts/research_experience/","summary":"Courses: Generalized Linear Model, Statistical Learning, Mathematical Finance, operational research, advanced algebra, mathematical analysis,stochastic analysis, probability theory.\nInterest: Application of machine learning like large language models on finance.\nResearch Projects Realted to Mathematics and Finance Research Proposal: Systematic Risk Discovery in High-Frequency Financial Data The study aims to explore methods for identifying risk factors in high-frequency trading data, focusing on market equilibrium analysis\n• Reviewed existing research on risk-return patterns in both high and low frequency trading, with emphasis on how market microstructure affects asset returns","title":"Research Experience"},{"content":"This Blog is devided into 2 parts. First part is the review of discussion of Emergence. You can just skip part 2 if you are not interested in theoretical staff. Second part illustrates the theory of emergence of complex skills in language model in a way people can understand easily, which is based on (Sanjeev et al. 2023) and the modified version of the paper Prof Arora (the author of the paper) sent me. Many thanks for the modified version of the paper and his comments on this blog.\nThe motivation for this blog is to make the relating theory easy for people to understand, so I put more illustration work on this blog but not omit some important theoretical staff. All of the formal definitions, assumptions and theorems in this paper are from those two resources above and other relating papers that I will refer later.\nThe first contribution is that I explained a few concepts in the paper in more detail. For example, in the 1st part of the blog, I gave a full review about what is emergence and what is the emergence of complex skills which the authors didn\u0026rsquo;t explain well. In the 2nd part, I wrote down the intuitions of the main theorems and how they were connected to the LLM which was not covered in the original paper.\nThe second contribution is that I added the proofs of a few statements which the author skipped in his paper (see Appendix 3.1, 3.2). I also reconstructed the proof of some theorems in a simpler way(see Appendix 3.3).\nConsidering the paper (Sanjeev et al. 2023) is not finalized, thus this blog cannot be yet finalized,too. I will keep it updated once I recieved more information from Professor Arora.\nPart 1: Discussion of Emergence The exsitance of emergence has been a hated topic recently. The debate mainly comes from different definition of emergence, thus we will review all of the definitions in the following.\n1 Real Defintion of Emergence Emergence is a popular phenomena in domains such as physics, biology, etc. The general definition of emergence, adapted from Steinhardt (2022) and rooted in a 1972 essay called “More Is Different” by Nobel prize-winning physicist Philip Anderson (Anderson, 1972):\nQuantitative changes in a system result in qualitative changes in behavior. The emergent abilities of large language models also attract lots of attention recentlly. However, the official definition of Emergence of Language Models remains in hated discussion.\n1.1 Outdated Definition of Emergence (Wei et al. 2022) first introduced emergent abilities of large language models, it was defined as following:\nSharpness, transitioning seemingly instantaneously from not present to present. Unpredictability, transitioning at seemingly unforeseeable model scales Figure 1: see (Wei et al. 2022). Eight examples of emergence in the few-shot prompting setting. Each point is a separate model. The ability to perform a task via few-shot prompting is emergent when a language model achieves random performance until a certain scale, after which performance significantly increases to well-above random.\nFigure 2:see (Wei et al. 2022). Analogous figure with number of model parameters instead of training flops in figure 1. Obviously it shows Figure 1 and figure 2 have the exactly same performance, which means models that used more training compute also typically have more parameters—hence.\n1.2 Is Emergence a Mirage? However, there was a time that people both from industry and academia doubted the existance of Emergent Abilities for language models. They argued those models actually do not show Sharpness,transitioning, unpredictability performance improvement when the model is scaled up, thus do not have Emergent Abilities.\nThere is no doubt the model gets better performance with more number of parameters and training flops.\nBut is the better performance just the sum of small improvement of each scaling up stage or is it greater than sum of those? Whether the sharp transition of the performance curve come from the choice of evaluation of metrics? see more in (Emergent Abilities Are a Mirage) Figure 3:see (Schaeffer et al. 2023). Claimed emergent abilities evaporate upon changing the metric. Left to Right: Mathematical Model, 2-Integer 2-Digit Multiplication Task, 2-Integer 4-Digit Addition Task. Top: When performance is measured by a nonlinear metric (e.g., Accuracy), the InstructGPT/GPT-3 [3, 24] family’s performance appears sharp and unpredictable on longer target lengths. Bottom: When performance is instead measured by a linear metric (e.g., Token Edit Distance), the family exhibits smooth, predictable performance improvements.\n(Schaeffer et al. 2023) conducted lots of experiments to prove that the sharpness, transitioning and unpredictability performance improvment of large language model is a creation of chosen metrics. Typically, linear or continuous metrics produce smooth,continuous,predictable changes in model performance, while a nonlinear or discontinuous metric can distort the model family’s performance from continuous, predictable changes to appear sharp, unpredictable.\nHowever, the explanation of metric choice above cannot be the evidence to deny the exsitance of emergent abilities. Since eventhough the performance does not show sharp transitioning, the improvement is still unpredictable to some extent.\nAs picture above shows, the performance improvement does not follow a deterministic function of scaling , which means it is unpredictable.\nAlso, the increasing rate of performance improvement is getting bigger and bigger, which means the performance improvement is greater than sum of small improvement of each scaling up stage, which obviously support that new skills will be emergent in large models.\n1.3 Latest Defition of Emergence Slow emergence is more widely accepted recently, meaning that the performance curve does not have to appear as a sharp transition when the model is scaled up. The sharp transition might be due to the final step metric evaluation method we used for meausring model performance. That is: solving one task reguires several steps, the models performance is actually incresing in each scaling step. but the evaluation metric just count for final step, which will result in the sharp transition of performance curve.\nEmergence is firstly introduced as Slow Emergence by (Sanjeev et al. 2023), which is defined as follows:\nAs number of sample sizes and number of parameters increase together then the model’s performance on a broad range of language tasks improves in a correlated way. I personally do not totally agree with the definition above. I agree that the performance curve does not have to be sharp transition part, but the performance increasement should be 1+1\u0026gt;2, which means the better performance is not the sum of small improvement of each scaling up stage, it should be greater than sum of those. however, the definition above does not illustrate 1+1\u0026gt;2.\nIn the paper by (Sanjeev et al. 2023), emergence of simple skills is defined as slow emergence above. Emergence of complex skills is defined as something new (those were not seen during training means new things) emerge\nThere are further discussion about emergence, (Lu et al. 2023) firstly mentioned Emergent abilities are In-context Learning. At the moment I will not extend this part.\nPart 2: theory of emergence of complex skills in language model I will first give a intuition about why complex skills emerge in language models. We have a text corpus, the the size of the training corpus is almost same with number of simple skills. We define complex skill as k\u0026rsquo;-tuple skill, which is composed of k\u0026rsquo; simple skills (you can find details about definitions about corpus and skills below).\nAssume we have 1000 training size and 1000 simple skills, 5 simple skills(k'=5) can compose a complex skill, thus we have \\(1000^{5}\\) =\\(10^{15}\\)combinations of complex skills. Since the training size is 1000, it can only see 1000/\\(10^{15}\\)=1/\\(10^{12}\\) proportion of complex skills during training(learning). We know that the model can only learn seen skills, if the model can only see 1/\\(10^{12}\\) proportion of complex skills during training, then it displays competence on complex skills is at most 1/\\(10^{12}\\) proportion. Thus if the model can display competency on even 10% proportion, which means the model emerges 10%-1/\\(10^{12}\\) complex skills despite it has never learned(seen during training) those complex skills. Thus, in order to show the emergence of complex skills, we need to show the competence of complex skills is at least not too small (bigger than e.g. 1/\\(10^{12}\\)), which means lots of new complex skills that are not seen during training process emerge. Here we introduce competence of simple skills to help us to quantify the competence of complex skills. If we can show that the competence of complex skill after scaling is at least the same level of competence of simple skills, which means the competence of complex skills is at least not too small since the competence of simple skills is not too small(since each simple skill will be seen thus be learned during training, so the competence of simple skills are not small).\nSo in part 2, the main goal is to quantify the competence of skills, then connect competence, scaling law and loss together to show that the competence of complex skill after scaling is at least the same level of competence of simple skills. Thus with the competence of complex skill bigger than the proportion of complex skills will be seen during training, we can conclude that the model can emerge new complex skills that are not seen during training when scaling up.\nThe structure of part 2 is as follows: we firstly introduce some basic knowledge for analyzing emergence in 2.1, 2.2 and 2.3. Then based on these, in 2.4.1 we analyze slow emergence(recall real definition of emergence in part 1) in language model from statistical view, which (Sanjeev et al. 2023) is as follows:\nAs the model’s excess cross entropy goes down due to scaling, the model’s performance improves.\nThen in 2.4.2 we continue introducing how complex skills(k\u0026rsquo; skills) emerge due to scaling, the key result as follows shows that if we scale up the model, the performance we get for complex skills is equal to performance we get for simple skills without scaling (Sanjeev et al. 2023):\nIf models of size S have a certain success rate (compentence) at solving tasks that require k\u0026rsquo; skills, then Scaling Laws imply that scaling up models (i.e. to size 10S leads to factor 2 reduction in loss) will give them the same success rate on tasks that involve combining 2k\u0026rsquo; skills as what we get on k\u0026rsquo; skills.\n2.1 Skills of Language Moldels First, we will clarify some terminologies required for this theory. There is no official definition for Skills of Language Model. Generally speaking, for those words generated by humanbeings or by neural network models, some skills are applied. (Sanjeev et al. 2023) develped a new theory as follows to describe skills as those for generating text pieces.\n2.1.1 Text Pieces Definition of Text Piece: (Sanjeev et al. 2023)\nThere is a measure \\(µ_{2}()\\) on these text-pieces, with \\(µ_{2}(t)\\) denoting the measure of text-piece t. The usual cross-entropy loss is computed by weighting text-pieces with respect to this measure. Assume we choose one piece of news from NewYork Times as our corpus, text piece should be thought of as having a size between a paragraph to a few pages, drawn from a longer corpus. Equivalently speaking, we can divide the news into text-pieces, each text pieces consitsts \\(C\\) tokens. We have train process and test process, thus corpus is devided as train corpus and test corpus, respectively. We will see in scling law introduced later that only test corpus is considered in this theory. --\u003e 2.1.2 Skills Definition of skill Graph: (Sanjeev et al. 2023)\nA skill graph is a bipartite graph \\((V_{1}, V_{2}, E)\\) where nodes in \\(V_{2}\\) correspond to skills, there is a measure \\(µ_{1}()\\) on these skills, with \\(µ_{1}(s)\\) denoting the measure of skill \\(s\\). Figure 4:see (Sanjeev et al. 2023). Each text piece can be generated by applying several skills.\nFor each text piece, skills are required for generating it. For the sentence above, What kinds of skills could it apply to generate the text pieces? You will have the answer after getting to know what do skills mean. We have example of skills as follows.\nNamed Entity Recognition (NER) pronoun disambiguation Sentiment Analysis Anaphora Resolution Logical Inference World Knowledge Barack Obama was born in Hawaii. He served as the 44th President of the United States. For the sentence above, Barack Obama is a name, Hawaii is a location, and the United States is a country, all of which can be determined by the skill Named Entity Recognition (NER). And by applying pronoun disambiguation can we infer He means Barack Obama.\nI love waiting in long lines. It makes me want to cut myself The sentence above is bit hard since it is an ironical expression, the word love here is not really love, it means hate instead, which requires the skill entiment analysis to analyze the sentiment of this expression is negative by the negative verb cut myself.\nThe CEO of the company made an announcement. It surprised everyone, so they get together to drink in the bar. It in the sentence means the thing he CEO of the company made an announcement, which requires the skill Anaphora Resolution. Since everyone is amazed by the god news, so we can infer everyone is happy, then they will celebrate in some place like in the bar, which requires the skill Logical Inference.\nThough I just list several skills for one sentence, the truth is when we generate the sentence, we have to apply almost every skills we want. the skills above are those named by human. However, there are more skills that are not found or named by human beings. For instance, those are lots of skills used in large language models like GPT, we human however cannot know each specific skills the model applies to generate text.\nAfter getting to know those examples above, a quick quiz for you: what kinds of skills could it apply to generate the following text pieces?\nThe city councilmen refused the demonstrators a permit because they feared violence.\n2.2 Cross Entropy Loss to Scaling Law Since we want to connect scaling law with skills, we can choose cross entropy as our bridge. We first find connection between cross entropy and scaling law, then we find connection between cross entropy and skills, thus we can find connection between scaling law and skills. So in this part, we are gonna first intriduce cross entropy then scaling law, then we will find connection between them.\n2.2.1 Cross Enrtopy For word generation, we always have a sequence of previous words to generate the next word, which is also named as next word prediction. Similar to other predicting process, language generating process also has 2 probability. One is the probability of the next predicted word of the current model, the other is the ground-truth probability (i.e. humans' choice), which can be understood as the true label for next word prediction process. Given the previous words \\(w_1 w_2 \\ldots w_i\\), the probability distribution of the model itself \\(q_{i}\\left(w\\right)\\), which can be defined as \\(\\operatorname{q}_i\\left[w \\mid w_1 w_2 \\ldots w_i\\right]\\). The ground truth distribution \\(p_{i}\\left(w\\right)\\) for generating the next \\((i + 1)th\\) word \\(w\\), which is defined as \\(\\operatorname{p}_i\\left[w \\mid w_1 w_2 \\ldots w_i\\right]\\). $$ p_{i}\\left(w\\right)\\ = \\begin{cases} 1, \u0026 \\text{if } w = w_{i+1} \\\\ 0. \u0026 \\text{if} w\\not =w_{i+1} \\end{cases}\\quad (1) $$ We take a particular interest in the difference between predicted word and the ground-truth word. The Cross Entropy can get a measure of dissimilarity between \\(p_{i}\\left(w\\right)\\) and \\(q_{i}\\left(w\\right)\\).Thus we introduce cross-entropy loss of the model on one word is: $$ \\sum_w p_i(w) \\log \\frac{1}{q_i(w)} \\quad(\\text {Cross Entropy of the word}) \\quad (2) $$ We have many attempts to make predictions, only one attempt is correct. That means only one attempt with ground-truth probability of 1, others are 0. So we sum all of the attempts (w) together to get the cross entropy on the ((i+1)th) word.\n$$ \\sum_w p_i(w_{i+1}) \\log \\frac{1}{q_i(w_{i+1})} = \\log \\frac{1}{q_i(w_{i+1})}\\quad (3) $$\nIt is easy to be confused between the Cross Entropy of the model and Cross Entropy of the word. Cross Entropy of the model is the total cross entropy of each word in the test corpus. After getting cross entropy of the word, we need to sum up all the cross entropy of the word to get the cross entropy of the model M.\n$$ \\ell(M)=\\sum_{i=1}^{N} \\log \\frac{1}{q_i(w_{i+1})} =-\\sum_{i=1}^{N} \\log q_i(w_{i+1}) \\quad (4) $$ which can be defined as follows: $$ =-\\sum_i \\log \\underset{M}{\\operatorname{Pr}}\\left[w_{i+1} \\mid w_1 w_2 \\ldots w_i\\right] \\quad \\text { (Cross Entropy of the Model) } \\quad (5) $$\nWe are also interested in the inherent property of language, we know human can have lots of choices for making the next word. Entropy can be used to describe the \"uncertainty\" inherent to the variable's possible outcomes. Thus we use Entropy to descibe inherent uncertainty of one word prediction: $$ \\sum_w p_i(w) \\log \\frac{1}{p_i(w)} \\quad(\\text {Entropy}) \\quad (6) $$ We also introduce KL divergence, which quantifies the information loss when the predicted distribution \\(q_{i}\\left(w\\right)\\) is used to approximate the true distribution \\(p_{i}\\left(w\\right)\\). it is also sometimes called excess entropy, is non-negative and defined as: $$ K L\\left(p_i \\| q_i\\right)=\\sum_w p_i(w) \\log \\frac{p_i(w)}{q_i(w)} \\quad(\\text {Excess Entropy}) \\quad (7) $$ Then we can find an intersting relationship on a per-word basis from equation\\((1),(2)\\) and \\((3)\\): $$ \\text { Corss Entropy }=\\text { Entropy }+ \\text { Excess Entropy} \\quad (8) $$ 2.2.2 Scaling Law Recall in part 1 we mentioned the real definition of Emergence:\nAs number of sample sizes and number of parameters increase together then the model’s performance on a broad range of language tasks improves in a correlated way.\nThe definition above reflects Scaling law, which describes how test cross entropy loss on test experiments scales with number of model parameters (N) and size of the dataset (D) Researchers have conducted lots of expiriments to derive Scaling law, Hoffmann et al. [2022] derived the scaling law is as follows:\n$$ L(N, D) = A + \\frac{B}{N^{0.34}} + \\frac{C}{D^{0.28}} \\quad A=1.61 \\quad B=406.4 \\quad C=410.7 \\quad (9) $$ 2.2.3 Understanding the Scaling Law in terms of excess entropy From [(Sanjeev et al. 2023)](https://arxiv.org/pdf/2307.15936), if we compare equation \\((9)\\) with \\((8)\\), the \\(A\\) term of \\((9)\\) captures the entropy of language. The lowest cross-entropy loss is entropy, that is term \\(A\\) for large corpora. The second and third terms of \\((9)\\) capture excess entropy, and they decrease polynomially with \\(N\\) and \\(D\\). For example when \\(D\\) is increased by a factor of 10 it reduces by roughly \\(10^{0.28} \\approx 2\\). This partly explains well the choice of metric evaluation could change the oerformance curve from sharp transition into slow increasement. As picture below shows: If we replace our loss metric from error rate as cross-entropy loss. We can observe the sharp transition of the perfromance curve disappear.\nThat is because the term \\(A\\) is a constant term, while the improvement of performance is brought by the sencond and third term. Thus, when there is some improvement for the sencond and third term, after we plus the first constant term \\(A\\), the curve will not show obvious improvement. Thus given the fact that excess cross entropy is the real driver in language generating process, it seems that we just need to consider **Excess Cross Entrioy** in our theory. Figure 5:see (Schaeffer et al. 2023). Adjacent plots for error rate, cross-entropy loss, and log probabilities of correct and incorrect responses on three classification tasks on BIG-Bench that we consider to demonstrate emergent abilities. Logical arguments only has 32 samples, which may contribute to noise. Error rate is (1 - accuracy).\n2.3 Skills modelling Skill modelling is a process of finding relationship between excess cross entropy and compentence on the skills. First we will make some assumptions, then we can quantify the model’s competence on particular skill.\n2.3.1 Mixing Assumption Each text-piece \\(t\\) can be generated by picking \\(k-tuple\\) of skills iid from measure \\(µ_{1}()\\) sing an unknown process, which assigns probability \\(µ_{2}(t)\\). Figure 6:Created by Xiaolan Liu. Skill graph where each text-piece t can be generated by picking k−tuple of skills .\nIt is assumed that we have a set of skills, which is pretty large and bit smaller than the number of text-pieces text pieces. Then text pieces are generated by picking random k-tuples of skills. l\n2.3.2 Scaling law Assumption The theory will assume scaling laws such as (9), thus it can reason directly about the model’s behavior on the test distribution We know the training corpus is quite large, which makes it hard to model our theory in training process. Luckily, the scaling laws help us avoid reasoning about training and generalization. The scaling law is based on test cross entropy loss, thus our theory does not need to refer to training cross entropy loss.\n2.3.3 Cloze Sufficiency Assumption To test the \\(k\\) underlying skills in \\(t\\), adds cloze prompts to \\(t\\) via an unknown process. The pre-trained model’s average prediction loss on Cloze questions (where the average is taken over the distribution of text pieces) closely tracks the excess cross-entropy of the model on classical next-word prediction.\nTheorem 1 in Appendix 3.1 justifies the exsitence of this assumption. You can find detailed math proof in Appendix.\nThe excess cross-entropy of the model on classical next-word prediction is not easy to calculate. Here we need to consider a equivalent method to get excess cross entropy loss, which is cloze questions approach. That means we add multiple choice question answering on text pieces to test the language model\u0026rsquo;s ability of understanding. Think the example below mentioned in (Sanjeev et al. 2023)\nThe city councilmen refused the demonstrators a permit because they feared violence.\nHere the pronoun they is ambiguous— grammar rules allow it to refer to either demonstrators or city councilmen. To test the model’s understanding of they in this sentence, we can append a prompt:\nQ. Who feared violence? A. city councilmen B.demonstrators\nAssume there are \\( N \\) text pieces in our model, we add \\( Q_i \\) cloze questions on this text piece above. The prediction loss on the text-piece above is the cross-entropy loss on predicting the answers to the cloze questions in it. The average prediction loss over all text-pieces is computed with respect to the measure \\(µ_{2}()\\), if we assume measure \\(µ_{2}()\\) is uniform. Then we have the average prediction loss over all text-pieces, noted as \\( \\delta \\), is defined as follows: \\[ \\delta = \\frac{1}{N} \\sum_{i=1}^N \\text{cross entropy on text piece } i \\] \\[ = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\text{cross entropy on cloze question } z_j^{(i)} \\] 2.3.4 compentence on the skills By cloze sufficiency assumption above, we replace model’s average prediction loss on Cloze questions as excess cross-entropy of the model on classical next-word prediction. Now we need to connect model’s average prediction loss on Cloze questions and compentence on the skills.\nFor a skill s, the competence of a model defined in the modified version of the paper Sanjeev sent to me: it is the expectation of the following random variable: randomly sample a text-piece containing that skill (this sampling uses the measure \\(µ_{2}(·)\\) on text-pieces) and measure the model’s success rate (1 - \\( \\delta \\)) at answering cloze questions in that text piece. We similarly define Competence on a tuple of skills (s1,s2,...,). 2.4 Analysis of Emergence Now we have set up a framework for modeling skills and connecting skills to the cross-entropy loss of the model, we firstly derive a mathematical formula for connections between loss, simple skills and scaling factor, then we have result: As the model’s excess cross entropy goes down due to scaling, the model’s performance on cloze tasks improves.\nAfter that, we derive a mathematical formula for connections between loss, complex skills and scaling factor, then we have result: the performance curve inferred by our method for k\u0026rsquo;-tuples of skills after scaling up is identical to the curve inferred for individual skills without scaling up.\n2.4.1 Emergence for Simple Skills Figure 7:Created by Xiaolan Liu. Skill graph where Y is the subset of such text pieces where the model makes mistakes on cloze questions.\n[(Sanjeev et al. 2023)](https://arxiv.org/pdf/2307.15936) mentioned how do we set up the theory in his paper: Let Y be the subset of such text pieces where the model makes mistakes on cloze questions. Let’s say the model makes a mistake on a text-piece if the total prediction loss on all the cloze questions of that text-piece is at least 1/2. Say Y contains \\(\\theta\\) fraction of text-pieces. Note that we cannot confuse \\(\\theta\\) with the average excess cross-entropy loss for those text-pieces. We have N1 text pieces, \\(\\theta\\)N1 of them make mistakes. It is easy to think that \\(\\theta\\)N1 /N = \\(\\theta\\) is equal to excess cross-entropy loss. However, Theorem 2 below shows those two are not equal. You can find the reconstruction of the proof in **Appendix 3.2.** Theorem 2 : if the average excess cross-entropy loss for the text-pieces is δ we conclude Y consists of at most 2δ fraction of text pieces.\nWe have theorem 3 below (you can find the proof in Appendix 3.3) guarantees that for most skills s, the model does not have significant error on the task associated with it. Note: theorem will give minimum guaranteed performance, actual performance could exceed this.\nTheorem 3: Let \\(\\alpha\\),\\(\\beta\\),\\(\\theta\\) \u003e 0, \\(\\beta\\)\u003e 1,\\(\\alpha\\beta\\) \u003c 1,\\(\\theta\\) \u003c 1 satisfy: \\[\\text{H}(\\theta) + k \\theta \\left( \\text{H}(\\beta \\alpha) - \\beta \\alpha \\log \\frac{1}{\\alpha} - (1-\\beta \\alpha) \\log \\left( \\frac{1}{1-\\alpha} \\right) \\right) = 0\\] where \\({H}(\\theta)\\) is Entropy defined as follows: \\[\\text{H}(x) = x \\log_2 \\frac{1}{x} + (1-x) \\log_2 \\frac{1}{1-x} \\tag{1}\\] We have the performance curve satisfying the theorem 3 as follows:\nFigure 8:see (Sanjeev et al. 2023). Performance Curves: The plot has theta = 0.1 and varies k = 2, 4, 8, 16. Higher values of k greatly improve performance.\nThe curve\\((k, \\theta)\\) above = set of \\((1-\\alpha,\\beta \\theta)\\) s.t. when excess c-e=\\(\\theta\\) then for \\(\\geq (1-\\alpha)\\) fraction of skills the model has error \\(\\leq \\beta \\theta\\) on the statistical task associated with that skill. For example, if \\(\\theta = 0.1\\), \\(\\alpha = 0.2\\), \\(\\beta = 3\\) \\(\\Rightarrow\\) For at least \\(1 - 0.2 = 0.8\\) fractions of skills, model answers incorrectly in at most 0.3 fraction of text pieces that used the skill. When we fix \\(\\theta\\), the emergence curves shift down noticeably (i.e., imply emergence of more skills) as we increase k. Note: please do not confuse k and k'. For simple skills, k means the number of simple skills that are required for generating text pieces. For complex skills, k also means the number of simple skills that are required for generating text pieces, but we have k' means the number of simple skills composed into complex skills. e.g., the perfromance curve shifts down if we increase the number of simple skills that are required for generating text pieces. But the performance curve shifts up(means more loss) for more complex skills(means bigger k\u0026rsquo;), see performance curve in 2.4.2.\nFigure 9: see (Sanjeev et al. 2023). Performance Curves: The plot has k = 8 and varies theta = 0.05, 0.1, 0.2.\nIf we fix k, when the model is scaled up, \\(\\theta\\) will go down, and the set Y containing erroneous answers on cloze questions will shrink. In terms of the emergence phenomenon, this corresponds to first signs of improvement of performance on tasks. 2.4.2 Emergence for Complex Skills We now derive emergence for tasks invloving complex (k\u0026rsquo;-tuples) skills. We have corollary 4 as follows:\nCorollary 4:In the same setting as Theorem 3 above,for integer \\(k' \\in [2, 1/\\theta]\\) the conclusion of that theorem holds also for \\(\\alpha, \\beta\\) pairs satisfying \\[\\text{H}(k'\\theta) + kk'\\theta \\left( \\text{H}(\\beta\\alpha) - \\beta\\alpha \\log \\frac{1}{\\alpha} - (1-\\beta\\alpha) \\log \\left( \\frac{1}{1-\\alpha} \\right) \\right) \u003c 0\\] Furthermore, if \\(\\text{H}(k'\\theta) \u003c k'\\text{H}(\\theta)\\) the emergence curve from this expression dominates that derived from Theorem 3 above. Figure 10: see (Sanjeev et al. 2023). Performance curve for t-tuples of skills for for theta = 0.05 and t = 1, 2, 4 respectively.\nBy the illustration in [(Sanjeev et al. 2023)](https://arxiv.org/pdf/2307.15936): From corollary 4, we can conclude that if \\(k'\\theta\\) is fixed, then we have same performance curve. It implies that the effect of reducing \\(\\theta\\) by a factor 2 has the effect of raising competence on 2k'-tuples to at least the same level as what it was on k'-tuples before reducing \\(\\theta\\). But how do we connect this with scaling law?\nBy the illustration in [(Sanjeev et al. 2023)](https://arxiv.org/pdf/2307.15936): Assume (for simplicity) a Chinchilla-like scaling law that 10x up-scaling leads to factor 2 reduction in excess entropy. By theorem 2, we know \\(\\theta\\) is proportional to excess cross entropy. So factor 2 reduction in \\(\\theta\\) corresponds factor 2 reduction in excess entrop. By the illustration in [(Sanjeev et al. 2023)](https://arxiv.org/pdf/2307.15936): If a model is considered to have reasonable performance on individual skill (1-tuple skills) at current scaling, then after further up-scaling of 10x(factor 2 reduction in \\(\\theta\\)) one would see similar reasonable performance on skill-pairs (2-tuples of skills), and scaling up by yet another 10x after that will yield similar reasonable performance on 4-tuples of skills, etc. Thus we have the result: the performance curve inferred by our method for k’-tuples of skills after scaling up is identical to the curve inferred for individual skills without scaling up. That means the competence of complex skills after scaling is not that samll, which means complex skills that were not seen during training must emerge.\nThe result is formalized as corollary 5 below: You can find the proof is in the paper.\nCorollary 5: When the model M1 with loss \\(\\delta\\) is scaled up (e.g., as per equation (3)) so that the new model M2 has loss \\(\\delta\\)/k', then the performance curve inferred by our method for k'-tuples of skills using M2 is identical to the curve inferred for individual skills on model M1. 2.4.3 Emergence analysis with general measure on text and skills Figure 11:Created by Xiaolan Liu. Skill graph where text pieces contain multiple skill clusters.\nMany skill clusters, not just one Each cluster has similar description as before, except skills of one cluster could be present in text in another cluster (e.g., ”Basic English” needed in ”Logic”)\nNothing much changes in theory, except now it only predicts emergence within a cluster if/when excess loss in that cluster goes down significantly.\nPart 3: Appendix 3.1 Theorem 1 If a model’s excess entropy at the ith place in text is ϵ then there is a cloze question with binary answer such that the probability that the model answers it incorrectly is at most \\(\\sqrt{2 \\epsilon}\\). Proof:\nDefine \\( p_i \\) to be the human's probability for the \\((i+1)\\)-th word, \\( q_i = \\) model's probability for the \\((i+1)\\)-th word. Step 1:\nShow that the probability of \\( p_i \\) and \\( q_i \\) giving different answers is \\( \\max_A \\left| \\sum_{A} \\left( p_i(w) - q_i(w) \\right) \\right| \\), where \\( A \\) is any subset of words. Pf of Step 1: Note that the human's probability \\( p_i \\) on \\((i+1)\\)-th word is: \\[ p_i(w) = P(W_{i+1} = w) = \\begin{cases} 1, \u0026 \\text{if } w = w^* \\\\ 0, \u0026 \\text{otherwise} \\end{cases} \\] i.e., \\( p_i(w^*) = 1 \\), \\( P_i(w) = 0 \\) if \\( w \\neq w^* \\) So \\( p_i \\) will always answer \\( w^* \\). The answer for \\( q_i(\\cdot) \\) is \\( \\arg \\max_w q_i(w) \\), denoted as \\( w^{**} \\). So in order to give different answers between \\( p_i \\) and \\( q_i \\), we need \\( w^* \\neq w^{**} \\). Step 2:\nFrom Step 1: $$ P(p_i \\text{ and } q_i \\text{ give different answers}) = \\text{variation distance between } p_i \\text{ and } g_i $$\nUsing Pinsker\u0026rsquo;s Theorem: $$ \\leq \\sqrt{\\frac{1}{2} KL(p_i \\Vert q_i)} \\leq \\sqrt{\\frac{1}{2} \\epsilon} $$\nConstruct the cloze question as follows:\nDefine \\(A_{i+1}\\) as: $$ A_{i+1} = \\arg \\max_A \\left| \\sum_A (p_i(w) - q_i(w)) \\right| $$ Then, the cloze question is: Is the \\((i+1)\\)-th word \\(w^*\\) in \\((a)\\), \\(A_{i+1}\\), or \\((b)\\), in \\(A_{i+1}^c\\)? $$ \\text{Pr(answer to cloze question is wrong)} = P(w^* \\in A_{i+1} \\text{ but we answer } (b)) + P(w^* \\in A_{i+1}^c \\text{ but we answer } (a)) $$\n$$ = P(p_i\u0026rsquo;s \\text{ answer is in } A_{i+1}, q_i\u0026rsquo;s \\text{ answer is in } A_{i+1}^c) + P(p_i\u0026rsquo;s \\text{ answer is in } A_{i+1}^c, q_i\u0026rsquo;s \\text{ answer is in } A_{i+1}) $$\n$$ \\leq P(p_i\u0026rsquo;s \\text{ answer is different from } q_i) + P(p_i\u0026rsquo;s \\text{ answer is different from } q_i) $$\n$$ = 2 P(p_i\u0026rsquo;s \\text{ answer is different from } q_i) $$\nIf we denote \\(p_i's \\text{ answer is in } A_{i+1}, q_i's \\text{ answer is in } A_{i+1}^c\\) as \\(E\\), \\(p_i's \\text{ answer is in } A_{i+1}^c, q_i's \\text{ answer is in } A_{i+1}\\) as \\(F\\), \\(p_i's \\text{ answer is different from } q_i\\) as \\(G\\). Since both \\(E\\) and \\(F\\) are subsets of \\(G\\), if \\(p_i's\\) answer in \\(A_{i+1}\\), \\(q_i's\\) answer in \\(A_{i+1}^c\\). Obviously answers are different, $$ \\Rightarrow E \\subseteq G, \\ F \\subseteq G $$\nThen by Pinsker\u0026rsquo;s Theorem above:\n$$ \\Rightarrow 2 \\cdot \\sqrt{\\frac{1}{2} \\epsilon} = \\sqrt{2 \\epsilon} $$\n3.2 Theorem2 Theorem 2 : if the average excess cross-entropy loss for the text-pieces is δ we conclude Y consists of at most 2δ fraction of text pieces.\nProof:\nAssume there are \\( N \\) text pieces, each text piece has \\( Q_i \\) cloze questions. \\( \\delta \\) is the average ce loss. \\[ \\delta = \\frac{1}{N} \\sum_{i=1}^N \\text{cross entropy on text piece } i \\] Since the ce loss on the text piece = the ce loss of cloze questions on this text piece,\n\\[ \\Rightarrow \\delta= \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\text{cross entropy on cloze question } z_j^{(i)} \\] Since The average ce loss \\(\\geq\\) the loss in subset \\(Y\\). \\[ \\Rightarrow \\delta \\geq \\frac{1}{N} \\sum_{i \\in Y} \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\text{cross entropy on cloze question } z_j^{(i)} := A_i \\] Step 1: Prove that \\( A_i \\geq \\frac{1}{2} \\) Step 2: Prove that \\( \\theta \\leq 2\\delta \\) Pf of step 1:\n\\[ A_i = \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\text{cross entropy on cloze question } z_j^{(i)} \\] \\[ = \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\sum_w -p_i(w) \\log q_i(w) \\] where \\(w\\) is all of the possible answers. \\[ = \\frac{1}{Q_i} \\left( \\sum_{j=1}^{Q_i} \\sum_w -p_i(w) \\log q_i(w) \\text{ (model is correct on } z_j^{(i)}) + \\sum_{j=1}^{Q_i} \\sum_w -p_i(w) \\log q_i(w) \\text{ (model is wrong on } z_j^{(i)})\\right) \\] \\[ \\geq \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\sum_w -p_i(w) \\log q_i(w)\\text{ (model is wrong on } z_j^{(i)}) \\] Assume \\(w*\\) is the correct answer. \\(\\Rightarrow p_i(w^*) = 1, p_i(w) = 0 \\text{ if } w \\ne w^*\\) \\[ \\Rightarrow \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} -\\log q_i(w^*) \\text{ (model is wrong on } z_j^{(i)}) \\] \\(\\text{Since model is wrong on } z_j^{(i)}\\) \\[ \\Rightarrow -\\log q_i(w^*) \\leq 1 \\quad (\\text{otherwise model will be correct}) \\] \\[ \\Rightarrow -\\log q_i(w^*) \\geq 1 \\] \\[ \\Rightarrow A_i = \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} \\left( -\\log q_i(w^*) \\right)\\text{(model is wrong on } z_j^{(i)}) \\] \\[ \\geq \\frac{1}{Q_i} \\sum_{j=1}^{Q_i} 1 \\text{(model is wrong on } z_j^{(i)}) \\] Define set \\( S_i = \\{z_j^{(i)} : \\text{model is wrong on } z_j^{(i)}\\} \\) Since \\(Y\\) is the set of text pieces with each has at least half of wrong cloze questions on it. \\[ \\text{by definition } \\Rightarrow |S_i| \\geq \\frac{1}{2} |Q_i| \\] \\[ \\Rightarrow A_i \\geq \\frac{1}{Q_i} |S_i| \\geq \\frac{1}{2} \\] Pf of step 2:\nSince \\( \\delta \\geq \\frac{1}{N} \\sum_{i \\in Y} A_i \\) \\[ \\geq \\frac{1}{N} \\sum_{i \\in Y} \\frac{1}{2} \\quad (\\text{from step 1, } A_i \\geq \\frac{1}{2}) \\] \\[ = \\frac{1}{2N} |Y| \\] \\[ = \\frac{1}{2N} \\cdot \\theta N \\] \\[ \\Rightarrow \\theta \\leq 2 \\delta \\] 3.3 Theorem 3 Theorem 3: Let \\(\\alpha\\),\\(\\beta\\),\\(\\theta\\) \u003e 0, \\(\\beta\\)\u003e 1,\\(\\alpha\\beta\\) \u003c 1,\\(\\theta\\) \u003c 1 satisfy: \\[\\text{H}(\\theta) + k \\theta \\left( \\text{H}(\\beta \\alpha) - \\beta \\alpha \\log \\frac{1}{\\alpha} - (1-\\beta \\alpha) \\log \\left( \\frac{1}{1-\\alpha} \\right) \\right) = 0\\] where \\({H}(\\theta)\\) is Entropy defined as follows: \\[\\text{H}(x) = x \\log_2 \\frac{1}{x} + (1-x) \\log_2 \\frac{1}{1-x} \\tag{1}\\] Proof:\nWe say model makes a mistake in a text piece if it fails to answer \u0026gt;= half of its cloze questions.\nSuppose \\(V1\\) is the set for text pieces and \\(V2\\) is the set for skills. Suppose \\(Y \\subseteq V1\\) of size \\(\\theta N1\\) is the subset of text pieces where model makes mistakes. We need to show that thare are at least \\((1-\\alpha)\\) fraction of vertices in \\(V2\\) each of which has at most \\(\\beta \\theta D\\) edges going to Y, where \\(D=\\frac{kN1}{N2}\\). Define a skill to be \"good\" if it has at most \\(\\beta\\theta D\\) edges going to Y, and \"bad\" if it has more than \\(\\beta\\theta D\\) edges going to Y. So we need to show that there are at least \\((1 - \\alpha) N2\\) good skills, or equivalently, show that there are at most \\(\\alpha N2\\) fraction bad skills. Define \\(Z \\subseteq V2, |Z2| \\leq \\alpha N2\\) be the subset of \\(V2\\) such that \\(Z\\) has at least \\(\\alpha \\beta \\theta kN1\\) edges to \\(Y\\). So \\(Z\\) is potentially the set for all \"bad\" skills. (Since \\((\\beta \\theta D) * (\\alpha N2) = \\alpha \\beta \\theta kN1)\\). If we can show that the expected number of such \\(Z\\)s is at most 1, then we can conclude the theorem because we have at most 1 such bad subset of skills, each has at most \\(\\alpha N2\\) bad skills, making it total at most \\(\\alpha N2\\) bad skills in \\(V2\\). By some combinatorics we conclude that the expected number of such \\(Z\\)s can be upper bounded by $$ N1N2\\binom{N2}{\\alpha N2}\\times \\binom{N1}{\\theta N1} \\times \\binom{k\\theta N_1}{\\beta \\alpha k \\theta N_1} \\times\\alpha^{\\beta\\alpha\\theta kN_1}\\times (1-\\alpha)^{(1-\\beta\\alpha)\\theta k N_1} $$ Note that the latter three terms is the probability in binomial distribution that among total \\(k\\theta N_1\\)outgoing edges from \\(Y\\), the probability of \\(\\beta \\alpha k\\theta N_1\\) of them connect to the vertices in Z. We need to show that the above formula can be upper bounded by 1, which can be shown by showing that the log of above is negative. Part 4: reference (Sanjeev et al. 2023)\n(Anderson, 1972)\n(Wei et al. 2022)\n(Schaeffer et al. 2023)\n(Lu et al. 2023)\n","permalink":"https://XIAOLAN-design.github.io/posts/emergence/","summary":"This Blog is devided into 2 parts. First part is the review of discussion of Emergence. You can just skip part 2 if you are not interested in theoretical staff. Second part illustrates the theory of emergence of complex skills in language model in a way people can understand easily, which is based on (Sanjeev et al. 2023) and the modified version of the paper Prof Arora (the author of the paper) sent me.","title":"Emergence of Complex Skills in Language Model"}]